# 2_Fine_Tuning_Process

This directory contains the scripts and resulting model adapters for the core two-stage fine-tuning process, as detailed in our paper **"Advancing Code Generation Large Language Models with Novel Fine-Tuning Techniques."**

This process is applied to each of the four base models: `gemma3-1b`, `gemma3-4b`, `gemma3-12b`, and `qwen3-4b`. The notebooks and adapters for each model are organised into their respective subfolders.

## Framework Overview

The fine-tuning workflow consists of two main stages, both of which utilise Direct Preference Optimisation (DPO) and are executed sequentially.

### Stage 1: Bootstrapping Alignment (Paper Stage 1c)

The first stage fine-tunes the base models on the **global DPO dataset** (created in the `1_Global_Dataset_Generation` workflow). This dataset, generated by a powerful external model, teaches the base models a general, foundational preference for maintainable code.

* **Script:** `1_[MODEL]_dpo.ipynb` (e.g., `1_gemma3_4b_dpo.ipynb`)
* **Input:** The base instruction-tuned model and the global DPO dataset (`mbpp_custom_dataset_with_correct_gemini_pairs_dpo_ready_with_messages`).
* **Process:** The script runs DPO fine-tuning using the [TRL library](https://huggingface.co/docs/trl/index). It employs Low-Rank Adaptation (LoRA) for parameter-efficient training.
* **Output:** The LoRA adapters for the once-tuned model (referred to as the **`-dpo`** model), saved in a directory like `gemma-3-4b-dpo-250-epochs`.

### Stage 2: Iterative Self-Tuning (Paper Stages 2a & 2c)

The second stage is a novel self-improvement loop where each model refines its own policy.

#### **Step 2a: Self-Improvement Data Generation**
First, each once-tuned (`-dpo`) model generates its *own* preference dataset.

* **Script:** `2_[MODEL]_dpo_local_dataset_self_generation.ipynb` (e.g., `2_gemma3_4b_dpo_local_dataset_self_generation.ipynb`)
* **Input:** The `-dpo` model adapters (from Stage 1) and the original `mbpp_preprocessed_dataset` prompts.
* **Process:** The `-dpo` model generates two solutions for each prompt. These pairs are filtered for functional correctness (both must pass unit tests) and then automatically labeled using the Maintainability Index (MI) to create a `(prompt, chosen, rejected)` dataset.
* **Output:** A new, model-specific "local" DPO dataset (e.g., `further_dpo_dataset_gemma3_4b_250_epochs`).

#### **Step 2c: Iterative Self-Tuning**
Next, the `-dpo` model is fine-tuned a second time, this time on its own self-generated "local" dataset.

* **Script:** `3_[MODEL]_dpo_further.ipynb` (e.g., `3_gemma3_4b_dpo_further.ipynb`)
* **Input:** The `-dpo` model adapters (from Stage 1) and the "local" DPO dataset (from Step 2a).
* **Process:** The script runs a second round of DPO fine-tuning using the same LoRA configuration. This allows the model to specialise and learn from its own stylistic tendencies and weaknesses, as identified by the MI metric.
* **Output:** The final LoRA adapters for the twice-tuned model (referred to as the **`-dpo-further`** model), saved in a directory like `gemma-3-4b-dpo-250-epochs-further-250-epochs`.

## Directory Structure

Each model-specific subfolder contains:

* `1_[MODEL]_dpo.ipynb`: Notebook for Stage 1 fine-tuning.
* `2_[MODEL]_dpo_local_dataset_self_generation.ipynb`: Notebook for Stage 2a data generation.
* `3_[MODEL]_dpo_further.ipynb`: Notebook for Stage 2c fine-tuning.
* **Dataset Folders**: Local copies of the input datasets (global DPO dataset, preprocessed prompts) and the output "local" DPO dataset.
* **Adapter Folders**: Directories containing the `adapter_model.safetensors` and `adapter_config.json` files for the `-dpo` and `-dpo-further` model variants.