{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import fileinput # Used to uncomment the execution line\n",
    "from typing import Iterable, Dict, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "MODEL_CACHE_DIRECTORY = \"./llm_models_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37666330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bcad9dc565e4dca851797f94a0596b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=MODEL_CACHE_DIRECTORY,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to trade compute for memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=MODEL_CACHE_DIRECTORY,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "test_dataset = load_from_disk(\"./mbpp_test_with_signatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486c6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code length: 512\n"
     ]
    }
   ],
   "source": [
    "# Check max token length of the chosen code\n",
    "max_code_length = max([len(tokenizer.encode(item['code'])) for item in test_dataset])\n",
    "print(f\"Max code length: {max_code_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87bdd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt: str) -> list[dict]:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a code generation model. Your task is to generate code snippets based on user prompts. You are to only write the full code for the required function and stop after returning the function output.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5d7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model on the test set with chat-templated prompts\n",
    "def evaluate_model(model, dataset, tokenizer, max_length=1024):\n",
    "    results = []\n",
    "\n",
    "    for item in tqdm(dataset):\n",
    "        results.append(\n",
    "            {\n",
    "                \"task_id\": item['task_id'],\n",
    "                \"generated_sequences\": []\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input_prompt = item['text']\n",
    "        messages = generate_prompt(input_prompt)\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_special_tokens = True,\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "            add_generation_prompt = True\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                top_p=0.9, # Adjust for more/less diversity\n",
    "                temperature=0.6, # Adjust for more/less diversity\n",
    "                num_return_sequences=10,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        for i in range(outputs.shape[0]):\n",
    "            decoded_output = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "            model_output = decoded_output.split(\"model\\n\")[-1].strip() # Extract the code part only\n",
    "            results[-1][\"generated_sequences\"].append(model_output)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af087633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4300079472\n"
     ]
    }
   ],
   "source": [
    "# Check total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after PEFT: 4332867952\n"
     ]
    }
   ],
   "source": [
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"./gemma-3-4b-dpo-250-epochs-further-250-epochs\",\n",
    "    device_map=\"cuda\"\n",
    "    )\n",
    "\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"Total parameters after PEFT: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e04e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the lora model\n",
    "for param in peft_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868bd22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation with the DPO finetuned model\n",
    "for i in range(1, 6):\n",
    "    dpo_results = evaluate_model(peft_model, test_dataset, tokenizer)\n",
    "    # Save the results to a JSON file\n",
    "    with open(f\"gemma_4b_dpo_250_further_250_model_output_run_{i}.json\", \"w\") as f:\n",
    "        json.dump(dpo_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
