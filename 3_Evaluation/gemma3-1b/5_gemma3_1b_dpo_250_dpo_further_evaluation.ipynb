{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import fileinput # Used to uncomment the execution line\n",
    "from typing import Iterable, Dict, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "MODEL_CACHE_DIRECTORY = \"./llm_models_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37666330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=MODEL_CACHE_DIRECTORY,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to trade compute for memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=MODEL_CACHE_DIRECTORY,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "test_dataset = load_from_disk(\"./mbpp_test_with_signatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486c6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max code length: 512\n"
     ]
    }
   ],
   "source": [
    "# Check max token length of the chosen code\n",
    "max_code_length = max([len(tokenizer.encode(item['code'])) for item in test_dataset])\n",
    "print(f\"Max code length: {max_code_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a87bdd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt: str) -> list[dict]:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a code generation model. Your task is to generate code snippets based on user prompts. You are to only write the full code for the required function and stop after returning the function output.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b5d7ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model on the test set with chat-templated prompts\n",
    "def evaluate_model(model, dataset, tokenizer, max_length=1024):\n",
    "    results = []\n",
    "\n",
    "    for item in tqdm(dataset):\n",
    "        results.append(\n",
    "            {\n",
    "                \"task_id\": item['task_id'],\n",
    "                \"generated_sequences\": []\n",
    "            }\n",
    "        )\n",
    "\n",
    "        input_prompt = item['text']\n",
    "        messages = generate_prompt(input_prompt)\n",
    "        \n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_special_tokens = True,\n",
    "            tokenize = True,\n",
    "            return_dict = True,\n",
    "            return_tensors = \"pt\",\n",
    "            add_generation_prompt = True\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                top_p=0.9, # Adjust for more/less diversity\n",
    "                temperature=0.6, # Adjust for more/less diversity\n",
    "                num_return_sequences=10,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        for i in range(outputs.shape[0]):\n",
    "            decoded_output = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "            model_output = decoded_output.split(\"model\\n\")[-1].strip() # Extract the code part only\n",
    "            results[-1][\"generated_sequences\"].append(model_output)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b44a352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters after PEFT: 1012931712\n"
     ]
    }
   ],
   "source": [
    "peft_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"./gemma-3-1b-dpo-250-epochs-further-250-epochs\",\n",
    "    device_map=\"cuda\"\n",
    "    )\n",
    "\n",
    "total_params = sum(p.numel() for p in peft_model.parameters())\n",
    "print(f\"Total parameters after PEFT: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e04e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the lora model\n",
    "for param in peft_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10fff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ccbe41f563e4937bd7b1268d98f06a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c352be71e2f84457a6d0c1b1d8c4ffbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c020cebce46839a08202fd7b411a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef31a0c15784bc9abfbdfc15486c6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291e3cfdc1634f659c25d2e99bba8d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b0381c67ba43f8b4872da61f8fda50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dd1e45fa9847e88ed51b1020cc8e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77b8124a7d74e84b01d02cbae45b635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284c655e2f124dde9684d50ee3041cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c54743f1d2d462e85c93ef82e03d67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b70248982b549baaf802904c2efe226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d5e8019fd14804be50895913286937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce90836355684d5785435ebfc0bf4c71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3960bdfa426c494f8dd73e8ff4915aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80408059e09d41bca93dccbeec1a5570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be152571307e4ab4a439a3eb458804bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707f20be14d94f778ea0748ddf7382f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae20c10f12d40ff8b18c6c14b2a0f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a482e5376d4c77bb9d4899c96f2a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4fd4d093454277bf24d7d3b0dc6b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform evaluation with the DPO finetuned model\n",
    "for i in range(1, 6):\n",
    "    dpo_results = evaluate_model(peft_model, test_dataset, tokenizer)\n",
    "    # Save the results to a JSON file\n",
    "    with open(f\"gemma_1b_dpo_250_further_250_model_output_run_{i}.json\", \"w\") as f:\n",
    "        json.dump(dpo_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868bd22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69191712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
